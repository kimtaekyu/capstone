{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":102262,"databundleVersionId":12341613,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:52:40.016567Z","iopub.execute_input":"2025-06-19T19:52:40.016891Z","iopub.status.idle":"2025-06-19T19:52:41.739277Z","shell.execute_reply.started":"2025-06-19T19:52:40.016865Z","shell.execute_reply":"2025-06-19T19:52:41.738448Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# 필요한 라이브러리 임포트\nimport os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\n# ---------- Dice Loss 함수 정의 ----------\n# 예측과 실제 정답 간의 겹치는 정도를 측정하는 Dice Loss 함수\ndef dice_loss(pred, target, smooth=1e-6):\n    pred = torch.softmax(pred, dim=1)[:, 1, :, :]  # 클래스 1에 대한 softmax 확률\n    target = (target == 1).float()                 # target도 1인 픽셀만 선택\n    intersection = (pred * target).sum(dim=(1, 2)) # 교집합\n    union = pred.sum(dim=(1, 2)) + target.sum(dim=(1, 2)) # 합집합\n    dice = (2. * intersection + smooth) / (union + smooth)\n    return 1 - dice.mean()\n\n# ---------- 데이터셋 클래스 ----------\nclass LaneDataset(Dataset):\n    def __init__(self, img_dir, mask_dir=None, transform=None):\n        self.img_dir = img_dir      # 이미지 디렉토리\n        self.mask_dir = mask_dir    # 마스크 디렉토리\n        self.images = sorted(os.listdir(img_dir))  # 이미지 파일 정렬\n        self.transform = transform\n\n    def __len__(self): \n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_name = self.images[idx]\n        img_path = os.path.join(self.img_dir, img_name)\n        pil_img = Image.open(img_path).convert('RGB')\n        original_size = pil_img.size  # (너비, 높이)\n        image = self.transform(pil_img)\n\n        if self.mask_dir:\n            # 학습/검증 시 마스크도 함께 불러옴\n            mask_path = os.path.join(self.mask_dir, img_name)\n            mask = Image.open(mask_path).convert('L').resize((1024, 512))\n            mask = (np.array(mask) > 127).astype(np.uint8)  # 이진화\n            mask = torch.tensor(mask, dtype=torch.long)\n            return image, mask\n        else:\n            # 테스트 시 마스크 없음 → 파일명과 원본 크기 반환\n            return image, img_name, original_size\n\n# ---------- 간단한 CNN 모델 정의 ----------\nclass SimpleCNNModel(nn.Module):\n    def __init__(self, n_class):\n        super().__init__()\n        # 인코더 (특징 추출)\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU()\n        )\n        # 디코더 (업샘플링)\n        self.decoder = nn.Sequential(\n            nn.Conv2d(256, 128, 3, padding=1), nn.ReLU(),\n            nn.ConvTranspose2d(128, 128, 2, 2), nn.ReLU(),\n            nn.Conv2d(128, 64, 3, padding=1), nn.ReLU(),\n            nn.ConvTranspose2d(64, 64, 2, 2), nn.ReLU(),\n            nn.Conv2d(64, 32, 3, padding=1), nn.ReLU()\n        )\n        # 클래스 분류기\n        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return self.classifier(x)\n\n# ---------- 마스크를 RLE로 인코딩하는 함수 ----------\ndef mask_to_rle(mask):\n    pixels = mask.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return \" \".join(map(str, runs))\n\n# ---------- 메인 함수 ----------\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # 이미지 전처리 정의\n    transform = transforms.Compose([\n        transforms.Resize((512, 1024)),\n        transforms.ToTensor()\n    ])\n\n    # --- 학습 시작 ---\n    train_dataset = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/train/train/frames\",\n        \"/kaggle/input/lanesegmentationchallenge/train/train/lane-masks\",\n        transform\n    )\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n\n    model = SimpleCNNModel(n_class=2).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n    ce_loss_fn = nn.CrossEntropyLoss()\n\n    print(\"▶️ 모델 학습 시작...\")\n    for epoch in range(1):\n        model.train()\n        total_loss = 0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n        for imgs, masks in pbar:\n            imgs, masks = imgs.to(device), masks.to(device)\n            outputs = model(imgs)\n            ce = ce_loss_fn(outputs, masks)\n            d  = dice_loss(outputs, masks)\n            loss = 0.5 * ce + 0.5 * d  \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n        print(f\"[Epoch {epoch+1}] 평균 손실: {total_loss / len(train_loader):.4f}\")\n\n    torch.save(model.state_dict(), \"simple_cnn_model.pth\")\n    print(\"💾 모델이 simple_cnn_model.pth로 저장되었습니다.\")\n\n    # --- 테스트 이미지에 대한 추론 및 제출 파일 생성 ---\n    print(\"🧪 submission.csv 생성 시작...\")\n    test_dataset = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/test/test/frames\",\n        None, transform\n    )\n    test_loader = DataLoader(test_dataset, batch_size=1)\n    model.eval()\n\n    results = []\n    with torch.no_grad():\n        for imgs, fnames, orig_size in tqdm(test_loader, desc=\"추론 중\"):\n            imgs = imgs.to(device)\n            output = model(imgs)\n            pred_mask = output.argmax(1).squeeze(0).cpu().numpy().astype(np.uint8)\n            pred_pil = Image.fromarray(pred_mask)\n            W, H = orig_size\n            resized_mask = pred_pil.resize((W, H), resample=Image.NEAREST)\n            binary = np.array(resized_mask) > 0\n            rle = mask_to_rle(binary)\n            results.append({'filename': fnames[0], 'rle': rle})\n\n    pd.DataFrame(results).to_csv(\"submission.csv\", index=False)\n    print(\"✅ submission.csv 파일이 저장되었습니다!\")\n\n# 프로그램 실행 진입점\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T20:09:27.149489Z","iopub.execute_input":"2025-06-19T20:09:27.149804Z","iopub.status.idle":"2025-06-19T20:16:37.468523Z","shell.execute_reply.started":"2025-06-19T20:09:27.149771Z","shell.execute_reply":"2025-06-19T20:16:37.467717Z"}},"outputs":[{"name":"stdout","text":"▶️ 모델 학습 시작...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 454/454 [04:44<00:00,  1.60it/s, loss=0.4136]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] 평균 손실: 0.5443\n💾 모델이 simple_cnn_model.pth로 저장되었습니다.\n🧪 submission.csv 생성 시작...\n","output_type":"stream"},{"name":"stderr","text":"추론 중: 100%|██████████| 2782/2782 [02:24<00:00, 19.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"✅ submission.csv 파일이 저장되었습니다!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:48:03.848411Z","iopub.execute_input":"2025-06-19T19:48:03.848765Z","iopub.status.idle":"2025-06-19T19:48:03.854041Z","shell.execute_reply.started":"2025-06-19T19:48:03.848722Z","shell.execute_reply":"2025-06-19T19:48:03.853235Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# 필요한 라이브러리 임포트\nimport os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\n\n# ---------- Dice Loss 함수 정의 ----------\n# 예측과 실제 정답 간의 겹치는 정도를 측정하는 Dice Loss 함수\ndef dice_loss(pred, target, smooth=1e-6):\n    pred = torch.softmax(pred, dim=1)[:, 1, :, :]  # 클래스 1에 대한 softmax 확률\n    target = (target == 1).float()                 # target도 1인 픽셀만 선택\n    intersection = (pred * target).sum(dim=(1, 2)) # 교집합\n    union = pred.sum(dim=(1, 2)) + target.sum(dim=(1, 2)) # 합집합\n    dice = (2. * intersection + smooth) / (union + smooth)\n    return 1 - dice.mean()\n\n# ---------- 데이터셋 클래스 ----------\nclass LaneDataset(Dataset):\n    def __init__(self, img_dir, mask_dir=None, transform=None):\n        self.img_dir = img_dir      # 이미지 디렉토리\n        self.mask_dir = mask_dir    # 마스크 디렉토리\n        self.images = sorted(os.listdir(img_dir))  # 이미지 파일 정렬\n        self.transform = transform\n\n    def __len__(self): \n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_name = self.images[idx]\n        img_path = os.path.join(self.img_dir, img_name)\n        pil_img = Image.open(img_path).convert('RGB')\n        original_size = pil_img.size  # (너비, 높이)\n        image = self.transform(pil_img)\n\n        if self.mask_dir:\n            # 학습/검증 시 마스크도 함께 불러옴\n            mask_path = os.path.join(self.mask_dir, img_name)\n            mask = Image.open(mask_path).convert('L').resize((1024, 512))\n            mask = (np.array(mask) > 127).astype(np.uint8)  # 이진화\n            mask = torch.tensor(mask, dtype=torch.long)\n            return image, mask\n        else:\n            # 테스트 시 마스크 없음 → 파일명과 원본 크기 반환\n            return image, img_name, original_size\n\n# ---------- 간단한 CNN 모델 정의 ----------\nclass SimpleCNNModel(nn.Module):\n    def __init__(self, n_class):\n        super().__init__()\n        # 인코더 (특징 추출)\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU()\n        )\n        # 디코더 (업샘플링)\n        self.decoder = nn.Sequential(\n            nn.Conv2d(256, 128, 3, padding=1), nn.ReLU(),\n            nn.ConvTranspose2d(128, 128, 2, 2), nn.ReLU(),\n            nn.Conv2d(128, 64, 3, padding=1), nn.ReLU(),\n            nn.ConvTranspose2d(64, 64, 2, 2), nn.ReLU(),\n            nn.Conv2d(64, 32, 3, padding=1), nn.ReLU()\n        )\n        # 클래스 분류기\n        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return self.classifier(x)\n# ---\nclass VGG16Seg(nn.Module):\n    def __init__(self, n_class=2, pretrained=True):\n        super().__init__()\n        vgg = models.vgg16(pretrained=pretrained)\n        self.encoder = vgg.features\n        self.classifier = nn.Conv2d(512, n_class, kernel_size=1)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.classifier(x)  # [B, n_class, H/32, W/32]\n        # Upsample back to input size\n        return nn.functional.interpolate(x, scale_factor=32, mode='bilinear', align_corners=False)\n# ---------- 마스크를 RLE로 인코딩하는 함수 ----------\ndef mask_to_rle(mask):\n    pixels = mask.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return \" \".join(map(str, runs))\n\n# ---------- 메인 함수 ----------\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # 이미지 전처리 정의\n    transform = transforms.Compose([\n        transforms.Resize((512, 1024)),\n        transforms.ToTensor()\n    ])\n\n    # --- 학습 시작 ---\n    train_dataset = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/train/train/frames\",\n        \"/kaggle/input/lanesegmentationchallenge/train/train/lane-masks\",\n        transform\n    )\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n\n    model = VGG16Seg(n_class=2).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n    ce_loss_fn = nn.CrossEntropyLoss()\n\n    print(\"▶️ 모델 학습 시작...\")\n    for epoch in range(1):\n        model.train()\n        total_loss = 0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n        for imgs, masks in pbar:\n            imgs, masks = imgs.to(device), masks.to(device)\n            outputs = model(imgs)\n            ce = ce_loss_fn(outputs, masks)\n            d  = dice_loss(outputs, masks)\n            loss = 0.5 * ce + 0.5 * d  \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n        print(f\"[Epoch {epoch+1}] 평균 손실: {total_loss / len(train_loader):.4f}\")\n\n    torch.save(model.state_dict(), \"simple_cnn_model.pth\")\n    print(\"💾 모델이 simple_cnn_model.pth로 저장되었습니다.\")\n\n    # --- 테스트 이미지에 대한 추론 및 제출 파일 생성 ---\n    print(\"🧪 submission.csv 생성 시작...\")\n    test_dataset = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/test/test/frames\",\n        None, transform\n    )\n    test_loader = DataLoader(test_dataset, batch_size=1)\n    model.eval()\n\n    results = []\n    with torch.no_grad():\n        for imgs, fnames, orig_size in tqdm(test_loader, desc=\"추론 중\"):\n            imgs = imgs.to(device)\n            output = model(imgs)\n            pred_mask = output.argmax(1).squeeze(0).cpu().numpy().astype(np.uint8)\n            pred_pil = Image.fromarray(pred_mask)\n            W, H = orig_size\n            resized_mask = pred_pil.resize((W, H), resample=Image.NEAREST)\n            binary = np.array(resized_mask) > 0\n            rle = mask_to_rle(binary)\n            results.append({'filename': fnames[0], 'rle': rle})\n\n    pd.DataFrame(results).to_csv(\"submission.csv\", index=False)\n    print(\"✅ submission.csv 파일이 저장되었습니다!\")\n\n# 프로그램 실행 진입점\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T20:23:37.456197Z","iopub.execute_input":"2025-06-19T20:23:37.456783Z","iopub.status.idle":"2025-06-19T20:31:28.564005Z","shell.execute_reply.started":"2025-06-19T20:23:37.456761Z","shell.execute_reply":"2025-06-19T20:31:28.563184Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:02<00:00, 222MB/s] \n","output_type":"stream"},{"name":"stdout","text":"▶️ 모델 학습 시작...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 454/454 [05:15<00:00,  1.44it/s, loss=0.2645]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] 평균 손실: 0.3075\n💾 모델이 simple_cnn_model.pth로 저장되었습니다.\n🧪 submission.csv 생성 시작...\n","output_type":"stream"},{"name":"stderr","text":"추론 중: 100%|██████████| 2782/2782 [02:30<00:00, 18.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"✅ submission.csv 파일이 저장되었습니다!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# 필요한 라이브러리 임포트\nimport os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\n\n# ---------- Dice Loss 함수 정의 ----------\n# 예측과 실제 정답 간의 겹치는 정도를 측정하는 Dice Loss 함수\ndef dice_loss(pred, target, smooth=1e-6):\n    pred = torch.softmax(pred, dim=1)[:, 1, :, :]  # 클래스 1에 대한 softmax 확률\n    target = (target == 1).float()                 # target도 1인 픽셀만 선택\n    intersection = (pred * target).sum(dim=(1, 2)) # 교집합\n    union = pred.sum(dim=(1, 2)) + target.sum(dim=(1, 2)) # 합집합\n    dice = (2. * intersection + smooth) / (union + smooth)\n    return 1 - dice.mean()\n\n# ---------- 데이터셋 클래스 ----------\nclass LaneDataset(Dataset):\n    def __init__(self, img_dir, mask_dir=None, transform=None):\n        self.img_dir = img_dir      # 이미지 디렉토리\n        self.mask_dir = mask_dir    # 마스크 디렉토리\n        self.images = sorted(os.listdir(img_dir))  # 이미지 파일 정렬\n        self.transform = transform\n\n    def __len__(self): \n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_name = self.images[idx]\n        img_path = os.path.join(self.img_dir, img_name)\n        pil_img = Image.open(img_path).convert('RGB')\n        original_size = pil_img.size  # (너비, 높이)\n        image = self.transform(pil_img)\n\n        if self.mask_dir:\n            # 학습/검증 시 마스크도 함께 불러옴\n            mask_path = os.path.join(self.mask_dir, img_name)\n            mask = Image.open(mask_path).convert('L').resize((1024, 512))\n            mask = (np.array(mask) > 127).astype(np.uint8)  # 이진화\n            mask = torch.tensor(mask, dtype=torch.long)\n            return image, mask\n        else:\n            # 테스트 시 마스크 없음 → 파일명과 원본 크기 반환\n            return image, img_name, original_size\n\n# ---------- 간단한 CNN 모델 정의 ----------\nclass SimpleCNNModel(nn.Module):\n    def __init__(self, n_class):\n        super().__init__()\n        # 인코더 (특징 추출)\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU()\n        )\n        # 디코더 (업샘플링)\n        self.decoder = nn.Sequential(\n            nn.Conv2d(256, 128, 3, padding=1), nn.ReLU(),\n            nn.ConvTranspose2d(128, 128, 2, 2), nn.ReLU(),\n            nn.Conv2d(128, 64, 3, padding=1), nn.ReLU(),\n            nn.ConvTranspose2d(64, 64, 2, 2), nn.ReLU(),\n            nn.Conv2d(64, 32, 3, padding=1), nn.ReLU()\n        )\n        # 클래스 분류기\n        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return self.classifier(x)\n# ---\nclass VGG16Seg(nn.Module):\n    def __init__(self, n_class=2, pretrained=True):\n        super().__init__()\n        vgg = models.vgg16(pretrained=pretrained)\n        self.encoder = vgg.features\n        self.classifier = nn.Conv2d(512, n_class, kernel_size=1)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.classifier(x)  # [B, n_class, H/32, W/32]\n        # Upsample back to input size\n        return nn.functional.interpolate(x, scale_factor=32, mode='bilinear', align_corners=False)\n# ---------- 마스크를 RLE로 인코딩하는 함수 ----------\nclass RESNET16Seg(nn.Module):\n    def __init__(self, n_class=2, pretrained=True):\n        super().__init__()\n        resnet = models.resnet18(pretrained=pretrained)\n        self.encoder = nn.Sequential(*list(resnet.children())[:-2])\n        self.classifier = nn.Conv2d(512, n_class, kernel_size=1)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.classifier(x)  # [B, n_class, H/32, W/32]\n        # Upsample back to input size\n        return nn.functional.interpolate(x, scale_factor=32, mode='bilinear', align_corners=False)\n#--\ndef mask_to_rle(mask):\n    pixels = mask.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return \" \".join(map(str, runs))\n\n# ---------- 메인 함수 ----------\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # 이미지 전처리 정의\n    transform = transforms.Compose([\n        transforms.Resize((512, 1024)),\n        transforms.ToTensor()\n    ])\n\n    # --- 학습 시작 ---\n    train_dataset = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/train/train/frames\",\n        \"/kaggle/input/lanesegmentationchallenge/train/train/lane-masks\",\n        transform\n    )\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n\n    model = RESNET16Seg(n_class=2).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n    ce_loss_fn = nn.CrossEntropyLoss()\n\n    print(\"▶️ 모델 학습 시작...\")\n    for epoch in range(1):\n        model.train()\n        total_loss = 0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n        for imgs, masks in pbar:\n            imgs, masks = imgs.to(device), masks.to(device)\n            outputs = model(imgs)\n            ce = ce_loss_fn(outputs, masks)\n            d  = dice_loss(outputs, masks)\n            loss = 0.5 * ce + 0.5 * d  \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n        print(f\"[Epoch {epoch+1}] 평균 손실: {total_loss / len(train_loader):.4f}\")\n\n    torch.save(model.state_dict(), \"simple_cnn_model.pth\")\n    print(\"💾 모델이 simple_cnn_model.pth로 저장되었습니다.\")\n\n    # --- 테스트 이미지에 대한 추론 및 제출 파일 생성 ---\n    print(\"🧪 submission.csv 생성 시작...\")\n    test_dataset = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/test/test/frames\",\n        None, transform\n    )\n    test_loader = DataLoader(test_dataset, batch_size=1)\n    model.eval()\n\n    results = []\n    with torch.no_grad():\n        for imgs, fnames, orig_size in tqdm(test_loader, desc=\"추론 중\"):\n            imgs = imgs.to(device)\n            output = model(imgs)\n            pred_mask = output.argmax(1).squeeze(0).cpu().numpy().astype(np.uint8)\n            pred_pil = Image.fromarray(pred_mask)\n            W, H = orig_size\n            resized_mask = pred_pil.resize((W, H), resample=Image.NEAREST)\n            binary = np.array(resized_mask) > 0\n            rle = mask_to_rle(binary)\n            results.append({'filename': fnames[0], 'rle': rle})\n\n    pd.DataFrame(results).to_csv(\"submission.csv\", index=False)\n    print(\"✅ submission.csv 파일이 저장되었습니다!\")\n\n# 프로그램 실행 진입점\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:19:03.233130Z","iopub.execute_input":"2025-06-19T21:19:03.233751Z","iopub.status.idle":"2025-06-19T21:22:12.921855Z","shell.execute_reply.started":"2025-06-19T21:19:03.233727Z","shell.execute_reply":"2025-06-19T21:22:12.920860Z"}},"outputs":[{"name":"stdout","text":"▶️ 모델 학습 시작...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 454/454 [01:33<00:00,  4.87it/s, loss=0.2442]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] 평균 손실: 0.3259\n💾 모델이 simple_cnn_model.pth로 저장되었습니다.\n🧪 submission.csv 생성 시작...\n","output_type":"stream"},{"name":"stderr","text":"추론 중: 100%|██████████| 2782/2782 [01:35<00:00, 29.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"✅ submission.csv 파일이 저장되었습니다!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\n# ---------- Dice Loss 함수 정의 ----------\ndef dice_loss(pred, target, smooth=1e-6):\n    pred_prob = torch.sigmoid(pred)\n    target = target.float()\n    intersection = (pred_prob * target).sum(dim=(1, 2))\n    union = pred_prob.sum(dim=(1, 2)) + target.sum(dim=(1, 2))\n    dice = (2. * intersection + smooth) / (union + smooth)\n    return 1 - dice.mean()\n\n# ---------- 데이터셋 클래스 ----------\nclass LaneDataset(Dataset):\n    def __init__(self, img_dir, mask_dir=None, transform=None):\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.images = sorted(os.listdir(img_dir))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_name = self.images[idx]\n        img_path = os.path.join(self.img_dir, img_name)\n        pil_img = Image.open(img_path).convert('RGB')\n        original_size = pil_img.size\n        image = self.transform(pil_img)\n\n        if self.mask_dir:\n            mask_path = os.path.join(self.mask_dir, img_name)\n            mask = Image.open(mask_path).convert('L').resize((1024, 512))\n            mask = (np.array(mask) > 127).astype(np.uint8)\n            mask = torch.tensor(mask, dtype=torch.float32)\n            return image, mask\n        else:\n            return image, img_name, original_size\n\n# ---------- 간단한 CNN 모델 정의 ----------\nclass SimpleCNNModel(nn.Module):\n    def __init__(self, n_class=1):\n        super().__init__()\n        # 인코더\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(256, 128, 3, padding=1), nn.ReLU()\n        )\n        # 디코더\n        self.decoder = nn.Sequential(\n            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),\n            nn.ConvTranspose2d(128, 128, 2, 2), nn.ReLU(),\n            nn.Conv2d(128, 64, 3, padding=1), nn.ReLU(),\n            nn.ConvTranspose2d(64, 64, 2, 2), nn.ReLU(),\n            nn.Conv2d(64, 32, 3, padding=1), nn.ReLU()\n        )\n        # 분류기\n        self.classifier = nn.Conv2d(32, 1, kernel_size=1)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return self.classifier(x).squeeze(1)\n\n# ---------- Semi-Supervised 학습 함수 ----------\ndef train_semi(model, labeled_loader, unlabeled_loader, optimizer, device, lambda_u=0.5):\n    model.train()\n    total_loss = 0.0\n    unl_iter = iter(unlabeled_loader)\n    bce = nn.BCEWithLogitsLoss()\n    for imgs_l, masks in tqdm(labeled_loader, desc='Semi-Train'):\n        imgs_l, masks = imgs_l.to(device), masks.to(device)\n        # Supervised loss\n        out_l = model(imgs_l)\n        loss_sup = bce(out_l, masks) + dice_loss(out_l, masks)\n        # Unsupervised pseudo-labeling\n        try:\n            imgs_u, _, _ = next(unl_iter)\n        except StopIteration:\n            unl_iter = iter(unlabeled_loader)\n            imgs_u, _, _ = next(unl_iter)\n        imgs_u = imgs_u.to(device)\n        with torch.no_grad():\n            pseudo = torch.sigmoid(model(imgs_u))\n        mask_p = (pseudo > 0.9).float()\n        loss_unsup = bce(model(imgs_u), mask_p)\n        # Total loss\n        loss = loss_sup + lambda_u * loss_unsup\n        model.zero_grad()\n        loss.backward()\n        optimizer.step()\n        torch.cuda.empty_cache()\n        total_loss += loss.item()\n    return total_loss / len(labeled_loader)\n\n# ---------- RLE 인코딩 ----------\ndef mask_to_rle(mask):\n    pixels = mask.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(map(str, runs))\n\n# ---------- 메인 ----------\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    transform = transforms.Compose([\n        transforms.Resize((512, 1024)),\n        transforms.ToTensor()\n    ])\n    # 데이터로더\n    labeled_ds = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/train/train/frames\",\n        \"/kaggle/input/lanesegmentationchallenge/train/train/lane-masks\",\n        transform\n    )\n    unlabeled_ds = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/test/test/frames\",\n        None, transform\n    )\n    labeled_loader = DataLoader(labeled_ds, batch_size=1, shuffle=True, num_workers=4)\n    unlabeled_loader = DataLoader(unlabeled_ds, batch_size=1, shuffle=True, num_workers=4)\n\n    # 모델 및 옵티마이저\n    model = SimpleCNNModel().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n    # Semi-supervised 학습\n    epochs = 3\n    print(\"▶️ Semi-supervised 학습 시작...\")\n    for ep in range(epochs):\n        loss = train_semi(model, labeled_loader, unlabeled_loader, optimizer, device)\n        print(f\"Epoch {ep+1}/{epochs} - Loss: {loss:.4f}\")\n    torch.save(model.state_dict(), \"semi_cnn_model.pth\")\n    print(\"💾 Semi-supervised 모델 저장 완료\")\n\n    # --- 테스트 및 제출 ---\n    print(\"🧪 submission.csv 생성 시작...\")\n    model.eval()\n    results = []\n    os.makedirs('pred_masks', exist_ok=True)\n    with torch.no_grad():\n        for imgs, fnames, orig_size in tqdm(DataLoader(unlabeled_ds, batch_size=1), desc=\"추론 중\"):\n            imgs = imgs.to(device)\n            output = model(imgs)\n            pred_mask = (torch.sigmoid(output).squeeze(0).cpu().numpy() > 0.5).astype(np.uint8)\n            W, H = orig_size\n            pred_pil = Image.fromarray(pred_mask * 255)\n            resized_mask = pred_pil.resize((W, H), resample=Image.NEAREST)\n            resized_bin = np.array(resized_mask) > 127\n            rle = mask_to_rle(resized_bin.astype(np.uint8))\n            # 예측 마스크 저장\n            mask_path = os.path.join('pred_masks', fnames[0])\n            pred_pil.save(mask_path)\n            results.append({'filename': fnames[0], 'rle': rle})\n    pd.DataFrame(results).to_csv(\"submission.csv\", index=False)\n    print(\"✅ submission.csv 파일이 저장되었습니다!\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:48:52.883654Z","iopub.execute_input":"2025-06-19T19:48:52.884192Z","iopub.status.idle":"2025-06-19T19:48:52.938654Z","shell.execute_reply.started":"2025-06-19T19:48:52.884169Z","shell.execute_reply":"2025-06-19T19:48:52.937696Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2977570784.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/2977570784.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# 모델 및 옵티마이저\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleCNNModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 33.12 MiB is free. Process 3330 has 15.85 GiB memory in use. Of the allocated memory 15.57 GiB is allocated by PyTorch, and 1.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 33.12 MiB is free. Process 3330 has 15.85 GiB memory in use. Of the allocated memory 15.57 GiB is allocated by PyTorch, and 1.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":19},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:51:41.232847Z","iopub.execute_input":"2025-06-19T19:51:41.233756Z","iopub.status.idle":"2025-06-19T19:51:41.238081Z","shell.execute_reply.started":"2025-06-19T19:51:41.233714Z","shell.execute_reply":"2025-06-19T19:51:41.237247Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\n# ---------- Dice Loss 함수 정의 ----------\ndef dice_loss(pred, target, smooth=1e-6):\n    prob = torch.sigmoid(pred)\n    inter = (prob * target).sum(dim=(1, 2))\n    union = prob.sum(dim=(1, 2)) + target.sum(dim=(1, 2))\n    dice = (2 * inter + smooth) / (union + smooth)\n    return 1 - dice.mean()\n\n# ---------- 데이터셋 클래스 ----------\nclass LaneDataset(Dataset):\n    def __init__(self, img_dir, mask_dir=None, transform=None):\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.images = sorted(os.listdir(img_dir))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_name = self.images[idx]\n        img = Image.open(os.path.join(self.img_dir, img_name)).convert('RGB')\n        img_t = self.transform(img) if self.transform else transforms.ToTensor()(img)\n        if self.mask_dir:\n            mask = Image.open(os.path.join(self.mask_dir, img_name)).convert('L')\n            mask = mask.resize((img_t.shape[2], img_t.shape[1]), Image.NEAREST)\n            mask_t = torch.from_numpy((np.array(mask) > 127).astype(np.float32))\n            return img_t, mask_t\n        else:\n            return img_t, img_name, img.size\n\n# ---------- 경량 CNN 모델 정의 ----------\nclass LightSegNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 인코더\n        self.enc1 = nn.Sequential(\n            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.enc2 = nn.Sequential(\n            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.enc3 = nn.Sequential(\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU()\n        )\n        # 디코더\n        self.dec3 = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, 2, stride=2), nn.ReLU()\n        )\n        self.dec2 = nn.Sequential(\n            nn.ConvTranspose2d(32, 16, 2, stride=2), nn.ReLU()\n        )\n        self.classifier = nn.Conv2d(16, 1, 1)\n\n    def forward(self, x):\n        x1 = self.enc1(x)\n        x2 = self.enc2(x1)\n        x3 = self.enc3(x2)\n        d2 = self.dec3(x3)\n        d1 = self.dec2(d2)\n        out = self.classifier(d1)\n        return out.squeeze(1)\n\n# ---------- Semi-Supervised 학습 함수 ----------\ndef train_semi(model, labeled_loader, unlabeled_loader, optimizer, device, lambda_u=0.5):\n    model.train()\n    total_loss = 0.0\n    unl_iter = iter(unlabeled_loader)\n    bce = nn.BCEWithLogitsLoss()\n    for imgs_l, masks in tqdm(labeled_loader, desc='Semi-Train'):\n        imgs_l, masks = imgs_l.to(device), masks.to(device)\n        out_l = model(imgs_l)\n        loss_sup = bce(out_l, masks) + dice_loss(out_l, masks)\n        try:\n            imgs_u, _, _ = next(unl_iter)\n        except StopIteration:\n            unl_iter = iter(unlabeled_loader)\n            imgs_u, _, _ = next(unl_iter)\n        imgs_u = imgs_u.to(device)\n        with torch.no_grad():\n            pseudo = torch.sigmoid(model(imgs_u))\n        mask_p = (pseudo > 0.9).float()\n        loss_unsup = bce(model(imgs_u), mask_p)\n        loss = loss_sup + lambda_u * loss_unsup\n        model.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if device.type == 'cuda':\n            torch.cuda.empty_cache()\n        total_loss += loss.item()\n    return total_loss / len(labeled_loader)\n\n# ---------- RLE 인코딩 ----------\ndef mask_to_rle(mask):\n    pixels = mask.flatten(order='F')\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(map(str, runs))\n\n# ---------- 메인 ----------\ndef main():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    transform = transforms.Compose([\n        transforms.Resize((256, 512)),\n        transforms.ToTensor()\n    ])\n    # 데이터로더\n    labeled_ds = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/train/train/frames\",\n        \"/kaggle/input/lanesegmentationchallenge/train/train/lane-masks\",\n        transform\n    )\n    unlabeled_ds = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/test/test/frames\",\n        None, transform\n    )\n    labeled_loader = DataLoader(labeled_ds, batch_size=1, shuffle=True, num_workers=2)\n    unlabeled_loader = DataLoader(unlabeled_ds, batch_size=1, shuffle=True, num_workers=2)\n\n    # 모델 및 옵티마이저\n    model = LightSegNet().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n\n    # Semi-supervised 학습\n    epochs = 10\n    print(\"▶️ Semi-supervised 학습 시작...\")\n    for ep in range(1, epochs+1):\n        loss = train_semi(model, labeled_loader, unlabeled_loader, optimizer, device)\n        print(f\"Epoch {ep}/{epochs} - Loss: {loss:.4f}\")\n    torch.save(model.state_dict(), \"semi_lightseg.pth\")\n    print(\"💾 Semi-supervised 모델 저장 완료\")\n\n    # --- 테스트 및 제출 ---\n    print(\"🧪 submission.csv 생성 시작...\")\n    model.eval()\n    results = []\n    os.makedirs('pred_masks', exist_ok=True)\n    with torch.no_grad():\n        for imgs, fnames, orig_size in tqdm(DataLoader(unlabeled_ds, batch_size=1), desc=\"추론 중\"):\n            imgs = imgs.to(device)\n            out = model(imgs)\n            pred = (torch.sigmoid(out).squeeze(0).cpu().numpy() > 0.5).astype(np.uint8)\n            W, H = orig_size\n            pil = Image.fromarray(pred * 255)\n            pil.save(os.path.join('pred_masks', fnames[0]))\n            rle = mask_to_rle(pred)\n            results.append({'filename': fnames[0], 'rle': rle})\n    pd.DataFrame(results).to_csv(\"submission.csv\", index=False)\n    print(\"✅ submission.csv 파일이 저장되었습니다!\")\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:52:41.740951Z","iopub.execute_input":"2025-06-19T19:52:41.741301Z","iopub.status.idle":"2025-06-19T20:06:27.522412Z","shell.execute_reply.started":"2025-06-19T19:52:41.741282Z","shell.execute_reply":"2025-06-19T20:06:27.521662Z"}},"outputs":[{"name":"stdout","text":"▶️ Semi-supervised 학습 시작...\n","output_type":"stream"},{"name":"stderr","text":"Semi-Train: 100%|██████████| 3626/3626 [01:21<00:00, 44.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 - Loss: 0.9692\n","output_type":"stream"},{"name":"stderr","text":"Semi-Train: 100%|██████████| 3626/3626 [01:14<00:00, 48.66it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10 - Loss: 0.7978\n","output_type":"stream"},{"name":"stderr","text":"\nSemi-Train: 100%|██████████| 3626/3626 [01:15<00:00, 48.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10 - Loss: 0.7385\n","output_type":"stream"},{"name":"stderr","text":"Semi-Train: 100%|██████████| 3626/3626 [01:15<00:00, 48.33it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10 - Loss: 0.6986\n","output_type":"stream"},{"name":"stderr","text":"\nSemi-Train: 100%|██████████| 3626/3626 [01:16<00:00, 47.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10 - Loss: 0.6697\n","output_type":"stream"},{"name":"stderr","text":"Semi-Train: 100%|██████████| 3626/3626 [01:15<00:00, 48.09it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10 - Loss: 0.6493\n","output_type":"stream"},{"name":"stderr","text":"\nSemi-Train: 100%|██████████| 3626/3626 [01:14<00:00, 48.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10 - Loss: 0.6329\n","output_type":"stream"},{"name":"stderr","text":"Semi-Train: 100%|██████████| 3626/3626 [01:17<00:00, 46.75it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10 - Loss: 0.6191\n","output_type":"stream"},{"name":"stderr","text":"\nSemi-Train: 100%|██████████| 3626/3626 [01:17<00:00, 47.04it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10 - Loss: 0.6082\n","output_type":"stream"},{"name":"stderr","text":"\nSemi-Train: 100%|██████████| 3626/3626 [01:15<00:00, 47.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10 - Loss: 0.5978\n💾 Semi-supervised 모델 저장 완료\n🧪 submission.csv 생성 시작...\n","output_type":"stream"},{"name":"stderr","text":"추론 중: 100%|██████████| 2782/2782 [00:54<00:00, 50.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"✅ submission.csv 파일이 저장되었습니다!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\n# ---------- Dice Loss 함수 정의 ----------\ndef dice_loss(pred, target, smooth=1e-6):\n    prob = torch.sigmoid(pred)\n    inter = (prob * target).sum(dim=(1, 2))\n    union = prob.sum(dim=(1, 2)) + target.sum(dim=(1, 2))\n    dice = (2 * inter + smooth) / (union + smooth)\n    return 1 - dice.mean()\n\n# ---------- 데이터셋 클래스 ----------\nclass LaneDataset(Dataset):\n    def __init__(self, img_dir, mask_dir=None, transform=None):\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.images = sorted(os.listdir(img_dir))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_name = self.images[idx]\n        img = Image.open(os.path.join(self.img_dir, img_name)).convert('RGB')\n        img_t = self.transform(img) if self.transform else transforms.ToTensor()(img)\n        if self.mask_dir:\n            mask = Image.open(os.path.join(self.mask_dir, img_name)).convert('L')\n            mask = mask.resize((img_t.shape[2], img_t.shape[1]), Image.NEAREST)\n            mask_t = torch.from_numpy((np.array(mask) > 127).astype(np.float32))\n            return img_t, mask_t\n        else:\n            return img_t, img_name, img.size\n\n# ---------- 경량 CNN 모델 정의 ----------\nclass LightSegNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 인코더\n        self.enc1 = nn.Sequential(\n            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.enc2 = nn.Sequential(\n            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.enc3 = nn.Sequential(\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU()\n        )\n        # 디코더\n        self.dec3 = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, 2, stride=2), nn.ReLU()\n        )\n        self.dec2 = nn.Sequential(\n            nn.ConvTranspose2d(32, 16, 2, stride=2), nn.ReLU()\n        )\n        self.classifier = nn.Conv2d(16, 1, 1)\n\n    def forward(self, x):\n        x1 = self.enc1(x)\n        x2 = self.enc2(x1)\n        x3 = self.enc3(x2)\n        d2 = self.dec3(x3)\n        d1 = self.dec2(d2)\n        out = self.classifier(d1)\n        return out.squeeze(1)\ndef train_supervised(model, data_loader, optimizer, device):\n    model.train()\n    total_loss = 0.0\n    bce = nn.BCEWithLogitsLoss()\n    for imgs, masks in tqdm(data_loader, desc='Supervised-Train'):\n        imgs, masks = imgs.to(device), masks.to(device)\n        out = model(imgs)\n        loss = bce(out, masks) + dice_loss(out, masks)\n        model.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if device.type == 'cuda':\n            torch.cuda.empty_cache()\n        total_loss += loss.item()\n    return total_loss / len(data_loader)\ndef main():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    transform = transforms.Compose([\n        transforms.Resize((256, 512)),\n        transforms.ToTensor()\n    ])\n    # 데이터로더\n    labeled_ds = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/train/train/frames\",\n        \"/kaggle/input/lanesegmentationchallenge/train/train/lane-masks\",\n        transform\n    )\n    unlabeled_ds = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/test/test/frames\",\n        None, transform\n    )\n    labeled_loader = DataLoader(labeled_ds, batch_size=1, shuffle=True, num_workers=2)\n    unlabeled_loader = DataLoader(unlabeled_ds, batch_size=1, shuffle=True, num_workers=2)\n\n    # 모델 및 옵티마이저\n    model = LightSegNet().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n\n    # Semi-supervised 학습\n    epochs = 10\n    print(\"▶️ Semi-supervised 학습 시작...\")\n    for ep in range(1, epochs+1):\n        loss = train_supervised(model, labeled_loader, unlabeled_loader, optimizer, device)\n        print(f\"Epoch {ep}/{epochs} - Loss: {loss:.4f}\")\n    torch.save(model.state_dict(), \"semi_lightseg.pth\")\n    print(\"💾 supervised 모델 저장 완료\")\n\n    # --- 테스트 및 제출 ---\n    print(\"🧪 submission.csv 생성 시작...\")\n    model.eval()\n    results = []\n    os.makedirs('pred_masks', exist_ok=True)\n    with torch.no_grad():\n        for imgs, fnames, orig_size in tqdm(DataLoader(unlabeled_ds, batch_size=1), desc=\"추론 중\"):\n            imgs = imgs.to(device)\n            out = model(imgs)\n            pred = (torch.sigmoid(out).squeeze(0).cpu().numpy() > 0.5).astype(np.uint8)\n            W, H = orig_size\n            pil = Image.fromarray(pred * 255)\n            pil.save(os.path.join('pred_masks', fnames[0]))\n            rle = mask_to_rle(pred)\n            results.append({'filename': fnames[0], 'rle': rle})\n    pd.DataFrame(results).to_csv(\"submission.csv\", index=False)\n    print(\"✅ submission.csv 파일이 저장되었습니다!\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}