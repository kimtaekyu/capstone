{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":102262,"databundleVersionId":12341613,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:52:40.016567Z","iopub.execute_input":"2025-06-19T19:52:40.016891Z","iopub.status.idle":"2025-06-19T19:52:41.739277Z","shell.execute_reply.started":"2025-06-19T19:52:40.016865Z","shell.execute_reply":"2025-06-19T19:52:41.738448Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\nimport os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\n# ---------- Dice Loss í•¨ìˆ˜ ì •ì˜ ----------\n# ì˜ˆì¸¡ê³¼ ì‹¤ì œ ì •ë‹µ ê°„ì˜ ê²¹ì¹˜ëŠ” ì •ë„ë¥¼ ì¸¡ì •í•˜ëŠ” Dice Loss í•¨ìˆ˜\ndef dice_loss(pred, target, smooth=1e-6):\n    pred = torch.softmax(pred, dim=1)[:, 1, :, :]  # í´ë˜ìŠ¤ 1ì— ëŒ€í•œ softmax í™•ë¥ \n    target = (target == 1).float()                 # targetë„ 1ì¸ í”½ì…€ë§Œ ì„ íƒ\n    intersection = (pred * target).sum(dim=(1, 2)) # êµì§‘í•©\n    union = pred.sum(dim=(1, 2)) + target.sum(dim=(1, 2)) # í•©ì§‘í•©\n    dice = (2. * intersection + smooth) / (union + smooth)\n    return 1 - dice.mean()\n\n# ---------- ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ----------\nclass LaneDataset(Dataset):\n    def __init__(self, img_dir, mask_dir=None, transform=None):\n        self.img_dir = img_dir      # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬\n        self.mask_dir = mask_dir    # ë§ˆìŠ¤í¬ ë””ë ‰í† ë¦¬\n        self.images = sorted(os.listdir(img_dir))  # ì´ë¯¸ì§€ íŒŒì¼ ì •ë ¬\n        self.transform = transform\n\n    def __len__(self): \n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_name = self.images[idx]\n        img_path = os.path.join(self.img_dir, img_name)\n        pil_img = Image.open(img_path).convert('RGB')\n        original_size = pil_img.size  # (ë„ˆë¹„, ë†’ì´)\n        image = self.transform(pil_img)\n\n        if self.mask_dir:\n            # í•™ìŠµ/ê²€ì¦ ì‹œ ë§ˆìŠ¤í¬ë„ í•¨ê»˜ ë¶ˆëŸ¬ì˜´\n            mask_path = os.path.join(self.mask_dir, img_name)\n            mask = Image.open(mask_path).convert('L').resize((1024, 512))\n            mask = (np.array(mask) > 127).astype(np.uint8)  # ì´ì§„í™”\n            mask = torch.tensor(mask, dtype=torch.long)\n            return image, mask\n        else:\n            # í…ŒìŠ¤íŠ¸ ì‹œ ë§ˆìŠ¤í¬ ì—†ìŒ â†’ íŒŒì¼ëª…ê³¼ ì›ë³¸ í¬ê¸° ë°˜í™˜\n            return image, img_name, original_size\n\n# ---------- ê°„ë‹¨í•œ CNN ëª¨ë¸ ì •ì˜ ----------\nclass SimpleCNNModel(nn.Module):\n    def __init__(self, n_class):\n        super().__init__()\n        # ì¸ì½”ë” (íŠ¹ì§• ì¶”ì¶œ)\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU()\n        )\n        # ë””ì½”ë” (ì—…ìƒ˜í”Œë§)\n        self.decoder = nn.Sequential(\n            nn.Conv2d(256, 128, 3, padding=1), nn.ReLU(),\n            nn.ConvTranspose2d(128, 128, 2, 2), nn.ReLU(),\n            nn.Conv2d(128, 64, 3, padding=1), nn.ReLU(),\n            nn.ConvTranspose2d(64, 64, 2, 2), nn.ReLU(),\n            nn.Conv2d(64, 32, 3, padding=1), nn.ReLU()\n        )\n        # í´ë˜ìŠ¤ ë¶„ë¥˜ê¸°\n        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return self.classifier(x)\n\n# ---------- ë§ˆìŠ¤í¬ë¥¼ RLEë¡œ ì¸ì½”ë”©í•˜ëŠ” í•¨ìˆ˜ ----------\ndef mask_to_rle(mask):\n    pixels = mask.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return \" \".join(map(str, runs))\n\n# ---------- ë©”ì¸ í•¨ìˆ˜ ----------\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì •ì˜\n    transform = transforms.Compose([\n        transforms.Resize((512, 1024)),\n        transforms.ToTensor()\n    ])\n\n    # --- í•™ìŠµ ì‹œì‘ ---\n    train_dataset = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/train/train/frames\",\n        \"/kaggle/input/lanesegmentationchallenge/train/train/lane-masks\",\n        transform\n    )\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n\n    model = SimpleCNNModel(n_class=2).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n    ce_loss_fn = nn.CrossEntropyLoss()\n\n    print(\"â–¶ï¸ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n    for epoch in range(1):\n        model.train()\n        total_loss = 0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n        for imgs, masks in pbar:\n            imgs, masks = imgs.to(device), masks.to(device)\n            outputs = model(imgs)\n            ce = ce_loss_fn(outputs, masks)\n            d  = dice_loss(outputs, masks)\n            loss = 0.5 * ce + 0.5 * d  \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n        print(f\"[Epoch {epoch+1}] í‰ê·  ì†ì‹¤: {total_loss / len(train_loader):.4f}\")\n\n    torch.save(model.state_dict(), \"simple_cnn_model.pth\")\n    print(\"ğŸ’¾ ëª¨ë¸ì´ simple_cnn_model.pthë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n\n    # --- í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì— ëŒ€í•œ ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„± ---\n    print(\"ğŸ§ª submission.csv ìƒì„± ì‹œì‘...\")\n    test_dataset = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/test/test/frames\",\n        None, transform\n    )\n    test_loader = DataLoader(test_dataset, batch_size=1)\n    model.eval()\n\n    results = []\n    with torch.no_grad():\n        for imgs, fnames, orig_size in tqdm(test_loader, desc=\"ì¶”ë¡  ì¤‘\"):\n            imgs = imgs.to(device)\n            output = model(imgs)\n            pred_mask = output.argmax(1).squeeze(0).cpu().numpy().astype(np.uint8)\n            pred_pil = Image.fromarray(pred_mask)\n            W, H = orig_size\n            resized_mask = pred_pil.resize((W, H), resample=Image.NEAREST)\n            binary = np.array(resized_mask) > 0\n            rle = mask_to_rle(binary)\n            results.append({'filename': fnames[0], 'rle': rle})\n\n    pd.DataFrame(results).to_csv(\"submission.csv\", index=False)\n    print(\"âœ… submission.csv íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n\n# í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì§„ì…ì \nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T20:09:27.149489Z","iopub.execute_input":"2025-06-19T20:09:27.149804Z","iopub.status.idle":"2025-06-19T20:16:37.468523Z","shell.execute_reply.started":"2025-06-19T20:09:27.149771Z","shell.execute_reply":"2025-06-19T20:16:37.467717Z"}},"outputs":[{"name":"stdout","text":"â–¶ï¸ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 454/454 [04:44<00:00,  1.60it/s, loss=0.4136]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] í‰ê·  ì†ì‹¤: 0.5443\nğŸ’¾ ëª¨ë¸ì´ simple_cnn_model.pthë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\nğŸ§ª submission.csv ìƒì„± ì‹œì‘...\n","output_type":"stream"},{"name":"stderr","text":"ì¶”ë¡  ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2782/2782 [02:24<00:00, 19.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"âœ… submission.csv íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:48:03.848411Z","iopub.execute_input":"2025-06-19T19:48:03.848765Z","iopub.status.idle":"2025-06-19T19:48:03.854041Z","shell.execute_reply.started":"2025-06-19T19:48:03.848722Z","shell.execute_reply":"2025-06-19T19:48:03.853235Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\nimport os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\n\n# ---------- Dice Loss í•¨ìˆ˜ ì •ì˜ ----------\n# ì˜ˆì¸¡ê³¼ ì‹¤ì œ ì •ë‹µ ê°„ì˜ ê²¹ì¹˜ëŠ” ì •ë„ë¥¼ ì¸¡ì •í•˜ëŠ” Dice Loss í•¨ìˆ˜\ndef dice_loss(pred, target, smooth=1e-6):\n    pred = torch.softmax(pred, dim=1)[:, 1, :, :]  # í´ë˜ìŠ¤ 1ì— ëŒ€í•œ softmax í™•ë¥ \n    target = (target == 1).float()                 # targetë„ 1ì¸ í”½ì…€ë§Œ ì„ íƒ\n    intersection = (pred * target).sum(dim=(1, 2)) # êµì§‘í•©\n    union = pred.sum(dim=(1, 2)) + target.sum(dim=(1, 2)) # í•©ì§‘í•©\n    dice = (2. * intersection + smooth) / (union + smooth)\n    return 1 - dice.mean()\n\n# ---------- ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ----------\nclass LaneDataset(Dataset):\n    def __init__(self, img_dir, mask_dir=None, transform=None):\n        self.img_dir = img_dir      # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬\n        self.mask_dir = mask_dir    # ë§ˆìŠ¤í¬ ë””ë ‰í† ë¦¬\n        self.images = sorted(os.listdir(img_dir))  # ì´ë¯¸ì§€ íŒŒì¼ ì •ë ¬\n        self.transform = transform\n\n    def __len__(self): \n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_name = self.images[idx]\n        img_path = os.path.join(self.img_dir, img_name)\n        pil_img = Image.open(img_path).convert('RGB')\n        original_size = pil_img.size  # (ë„ˆë¹„, ë†’ì´)\n        image = self.transform(pil_img)\n\n        if self.mask_dir:\n            # í•™ìŠµ/ê²€ì¦ ì‹œ ë§ˆìŠ¤í¬ë„ í•¨ê»˜ ë¶ˆëŸ¬ì˜´\n            mask_path = os.path.join(self.mask_dir, img_name)\n            mask = Image.open(mask_path).convert('L').resize((1024, 512))\n            mask = (np.array(mask) > 127).astype(np.uint8)  # ì´ì§„í™”\n            mask = torch.tensor(mask, dtype=torch.long)\n            return image, mask\n        else:\n            # í…ŒìŠ¤íŠ¸ ì‹œ ë§ˆìŠ¤í¬ ì—†ìŒ â†’ íŒŒì¼ëª…ê³¼ ì›ë³¸ í¬ê¸° ë°˜í™˜\n            return image, img_name, original_size\n\n# ---------- ê°„ë‹¨í•œ CNN ëª¨ë¸ ì •ì˜ ----------\nclass SimpleCNNModel(nn.Module):\n    def __init__(self, n_class):\n        super().__init__()\n        # ì¸ì½”ë” (íŠ¹ì§• ì¶”ì¶œ)\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU()\n        )\n        # ë””ì½”ë” (ì—…ìƒ˜í”Œë§)\n        self.decoder = nn.Sequential(\n            nn.Conv2d(256, 128, 3, padding=1), nn.ReLU(),\n            nn.ConvTranspose2d(128, 128, 2, 2), nn.ReLU(),\n            nn.Conv2d(128, 64, 3, padding=1), nn.ReLU(),\n            nn.ConvTranspose2d(64, 64, 2, 2), nn.ReLU(),\n            nn.Conv2d(64, 32, 3, padding=1), nn.ReLU()\n        )\n        # í´ë˜ìŠ¤ ë¶„ë¥˜ê¸°\n        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return self.classifier(x)\n# ---\nclass VGG16Seg(nn.Module):\n    def __init__(self, n_class=2, pretrained=True):\n        super().__init__()\n        vgg = models.vgg16(pretrained=pretrained)\n        self.encoder = vgg.features\n        self.classifier = nn.Conv2d(512, n_class, kernel_size=1)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.classifier(x)  # [B, n_class, H/32, W/32]\n        # Upsample back to input size\n        return nn.functional.interpolate(x, scale_factor=32, mode='bilinear', align_corners=False)\n# ---------- ë§ˆìŠ¤í¬ë¥¼ RLEë¡œ ì¸ì½”ë”©í•˜ëŠ” í•¨ìˆ˜ ----------\ndef mask_to_rle(mask):\n    pixels = mask.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return \" \".join(map(str, runs))\n\n# ---------- ë©”ì¸ í•¨ìˆ˜ ----------\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì •ì˜\n    transform = transforms.Compose([\n        transforms.Resize((512, 1024)),\n        transforms.ToTensor()\n    ])\n\n    # --- í•™ìŠµ ì‹œì‘ ---\n    train_dataset = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/train/train/frames\",\n        \"/kaggle/input/lanesegmentationchallenge/train/train/lane-masks\",\n        transform\n    )\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n\n    model = VGG16Seg(n_class=2).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n    ce_loss_fn = nn.CrossEntropyLoss()\n\n    print(\"â–¶ï¸ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n    for epoch in range(1):\n        model.train()\n        total_loss = 0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n        for imgs, masks in pbar:\n            imgs, masks = imgs.to(device), masks.to(device)\n            outputs = model(imgs)\n            ce = ce_loss_fn(outputs, masks)\n            d  = dice_loss(outputs, masks)\n            loss = 0.5 * ce + 0.5 * d  \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n        print(f\"[Epoch {epoch+1}] í‰ê·  ì†ì‹¤: {total_loss / len(train_loader):.4f}\")\n\n    torch.save(model.state_dict(), \"simple_cnn_model.pth\")\n    print(\"ğŸ’¾ ëª¨ë¸ì´ simple_cnn_model.pthë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n\n    # --- í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì— ëŒ€í•œ ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„± ---\n    print(\"ğŸ§ª submission.csv ìƒì„± ì‹œì‘...\")\n    test_dataset = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/test/test/frames\",\n        None, transform\n    )\n    test_loader = DataLoader(test_dataset, batch_size=1)\n    model.eval()\n\n    results = []\n    with torch.no_grad():\n        for imgs, fnames, orig_size in tqdm(test_loader, desc=\"ì¶”ë¡  ì¤‘\"):\n            imgs = imgs.to(device)\n            output = model(imgs)\n            pred_mask = output.argmax(1).squeeze(0).cpu().numpy().astype(np.uint8)\n            pred_pil = Image.fromarray(pred_mask)\n            W, H = orig_size\n            resized_mask = pred_pil.resize((W, H), resample=Image.NEAREST)\n            binary = np.array(resized_mask) > 0\n            rle = mask_to_rle(binary)\n            results.append({'filename': fnames[0], 'rle': rle})\n\n    pd.DataFrame(results).to_csv(\"submission.csv\", index=False)\n    print(\"âœ… submission.csv íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n\n# í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì§„ì…ì \nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T20:23:37.456197Z","iopub.execute_input":"2025-06-19T20:23:37.456783Z","iopub.status.idle":"2025-06-19T20:31:28.564005Z","shell.execute_reply.started":"2025-06-19T20:23:37.456761Z","shell.execute_reply":"2025-06-19T20:31:28.563184Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 528M/528M [00:02<00:00, 222MB/s] \n","output_type":"stream"},{"name":"stdout","text":"â–¶ï¸ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 454/454 [05:15<00:00,  1.44it/s, loss=0.2645]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] í‰ê·  ì†ì‹¤: 0.3075\nğŸ’¾ ëª¨ë¸ì´ simple_cnn_model.pthë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\nğŸ§ª submission.csv ìƒì„± ì‹œì‘...\n","output_type":"stream"},{"name":"stderr","text":"ì¶”ë¡  ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2782/2782 [02:30<00:00, 18.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"âœ… submission.csv íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\nimport os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\n\n# ---------- Dice Loss í•¨ìˆ˜ ì •ì˜ ----------\n# ì˜ˆì¸¡ê³¼ ì‹¤ì œ ì •ë‹µ ê°„ì˜ ê²¹ì¹˜ëŠ” ì •ë„ë¥¼ ì¸¡ì •í•˜ëŠ” Dice Loss í•¨ìˆ˜\ndef dice_loss(pred, target, smooth=1e-6):\n    pred = torch.softmax(pred, dim=1)[:, 1, :, :]  # í´ë˜ìŠ¤ 1ì— ëŒ€í•œ softmax í™•ë¥ \n    target = (target == 1).float()                 # targetë„ 1ì¸ í”½ì…€ë§Œ ì„ íƒ\n    intersection = (pred * target).sum(dim=(1, 2)) # êµì§‘í•©\n    union = pred.sum(dim=(1, 2)) + target.sum(dim=(1, 2)) # í•©ì§‘í•©\n    dice = (2. * intersection + smooth) / (union + smooth)\n    return 1 - dice.mean()\n\n# ---------- ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ----------\nclass LaneDataset(Dataset):\n    def __init__(self, img_dir, mask_dir=None, transform=None):\n        self.img_dir = img_dir      # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬\n        self.mask_dir = mask_dir    # ë§ˆìŠ¤í¬ ë””ë ‰í† ë¦¬\n        self.images = sorted(os.listdir(img_dir))  # ì´ë¯¸ì§€ íŒŒì¼ ì •ë ¬\n        self.transform = transform\n\n    def __len__(self): \n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_name = self.images[idx]\n        img_path = os.path.join(self.img_dir, img_name)\n        pil_img = Image.open(img_path).convert('RGB')\n        original_size = pil_img.size  # (ë„ˆë¹„, ë†’ì´)\n        image = self.transform(pil_img)\n\n        if self.mask_dir:\n            # í•™ìŠµ/ê²€ì¦ ì‹œ ë§ˆìŠ¤í¬ë„ í•¨ê»˜ ë¶ˆëŸ¬ì˜´\n            mask_path = os.path.join(self.mask_dir, img_name)\n            mask = Image.open(mask_path).convert('L').resize((1024, 512))\n            mask = (np.array(mask) > 127).astype(np.uint8)  # ì´ì§„í™”\n            mask = torch.tensor(mask, dtype=torch.long)\n            return image, mask\n        else:\n            # í…ŒìŠ¤íŠ¸ ì‹œ ë§ˆìŠ¤í¬ ì—†ìŒ â†’ íŒŒì¼ëª…ê³¼ ì›ë³¸ í¬ê¸° ë°˜í™˜\n            return image, img_name, original_size\n\n# ---------- ê°„ë‹¨í•œ CNN ëª¨ë¸ ì •ì˜ ----------\nclass SimpleCNNModel(nn.Module):\n    def __init__(self, n_class):\n        super().__init__()\n        # ì¸ì½”ë” (íŠ¹ì§• ì¶”ì¶œ)\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU()\n        )\n        # ë””ì½”ë” (ì—…ìƒ˜í”Œë§)\n        self.decoder = nn.Sequential(\n            nn.Conv2d(256, 128, 3, padding=1), nn.ReLU(),\n            nn.ConvTranspose2d(128, 128, 2, 2), nn.ReLU(),\n            nn.Conv2d(128, 64, 3, padding=1), nn.ReLU(),\n            nn.ConvTranspose2d(64, 64, 2, 2), nn.ReLU(),\n            nn.Conv2d(64, 32, 3, padding=1), nn.ReLU()\n        )\n        # í´ë˜ìŠ¤ ë¶„ë¥˜ê¸°\n        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return self.classifier(x)\n# ---\nclass VGG16Seg(nn.Module):\n    def __init__(self, n_class=2, pretrained=True):\n        super().__init__()\n        vgg = models.vgg16(pretrained=pretrained)\n        self.encoder = vgg.features\n        self.classifier = nn.Conv2d(512, n_class, kernel_size=1)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.classifier(x)  # [B, n_class, H/32, W/32]\n        # Upsample back to input size\n        return nn.functional.interpolate(x, scale_factor=32, mode='bilinear', align_corners=False)\n# ---------- ë§ˆìŠ¤í¬ë¥¼ RLEë¡œ ì¸ì½”ë”©í•˜ëŠ” í•¨ìˆ˜ ----------\nclass RESNET16Seg(nn.Module):\n    def __init__(self, n_class=2, pretrained=True):\n        super().__init__()\n        resnet = models.resnet18(pretrained=pretrained)\n        self.encoder = nn.Sequential(*list(resnet.children())[:-2])\n        self.classifier = nn.Conv2d(512, n_class, kernel_size=1)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.classifier(x)  # [B, n_class, H/32, W/32]\n        # Upsample back to input size\n        return nn.functional.interpolate(x, scale_factor=32, mode='bilinear', align_corners=False)\n#--\ndef mask_to_rle(mask):\n    pixels = mask.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return \" \".join(map(str, runs))\n\n# ---------- ë©”ì¸ í•¨ìˆ˜ ----------\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì •ì˜\n    transform = transforms.Compose([\n        transforms.Resize((512, 1024)),\n        transforms.ToTensor()\n    ])\n\n    # --- í•™ìŠµ ì‹œì‘ ---\n    train_dataset = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/train/train/frames\",\n        \"/kaggle/input/lanesegmentationchallenge/train/train/lane-masks\",\n        transform\n    )\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n\n    model = RESNET16Seg(n_class=2).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n    ce_loss_fn = nn.CrossEntropyLoss()\n\n    print(\"â–¶ï¸ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n    for epoch in range(1):\n        model.train()\n        total_loss = 0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n        for imgs, masks in pbar:\n            imgs, masks = imgs.to(device), masks.to(device)\n            outputs = model(imgs)\n            ce = ce_loss_fn(outputs, masks)\n            d  = dice_loss(outputs, masks)\n            loss = 0.5 * ce + 0.5 * d  \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n        print(f\"[Epoch {epoch+1}] í‰ê·  ì†ì‹¤: {total_loss / len(train_loader):.4f}\")\n\n    torch.save(model.state_dict(), \"simple_cnn_model.pth\")\n    print(\"ğŸ’¾ ëª¨ë¸ì´ simple_cnn_model.pthë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n\n    # --- í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì— ëŒ€í•œ ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„± ---\n    print(\"ğŸ§ª submission.csv ìƒì„± ì‹œì‘...\")\n    test_dataset = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/test/test/frames\",\n        None, transform\n    )\n    test_loader = DataLoader(test_dataset, batch_size=1)\n    model.eval()\n\n    results = []\n    with torch.no_grad():\n        for imgs, fnames, orig_size in tqdm(test_loader, desc=\"ì¶”ë¡  ì¤‘\"):\n            imgs = imgs.to(device)\n            output = model(imgs)\n            pred_mask = output.argmax(1).squeeze(0).cpu().numpy().astype(np.uint8)\n            pred_pil = Image.fromarray(pred_mask)\n            W, H = orig_size\n            resized_mask = pred_pil.resize((W, H), resample=Image.NEAREST)\n            binary = np.array(resized_mask) > 0\n            rle = mask_to_rle(binary)\n            results.append({'filename': fnames[0], 'rle': rle})\n\n    pd.DataFrame(results).to_csv(\"submission.csv\", index=False)\n    print(\"âœ… submission.csv íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n\n# í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì§„ì…ì \nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:19:03.233130Z","iopub.execute_input":"2025-06-19T21:19:03.233751Z","iopub.status.idle":"2025-06-19T21:22:12.921855Z","shell.execute_reply.started":"2025-06-19T21:19:03.233727Z","shell.execute_reply":"2025-06-19T21:22:12.920860Z"}},"outputs":[{"name":"stdout","text":"â–¶ï¸ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 454/454 [01:33<00:00,  4.87it/s, loss=0.2442]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] í‰ê·  ì†ì‹¤: 0.3259\nğŸ’¾ ëª¨ë¸ì´ simple_cnn_model.pthë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\nğŸ§ª submission.csv ìƒì„± ì‹œì‘...\n","output_type":"stream"},{"name":"stderr","text":"ì¶”ë¡  ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2782/2782 [01:35<00:00, 29.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"âœ… submission.csv íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\n# ---------- Dice Loss í•¨ìˆ˜ ì •ì˜ ----------\ndef dice_loss(pred, target, smooth=1e-6):\n    pred_prob = torch.sigmoid(pred)\n    target = target.float()\n    intersection = (pred_prob * target).sum(dim=(1, 2))\n    union = pred_prob.sum(dim=(1, 2)) + target.sum(dim=(1, 2))\n    dice = (2. * intersection + smooth) / (union + smooth)\n    return 1 - dice.mean()\n\n# ---------- ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ----------\nclass LaneDataset(Dataset):\n    def __init__(self, img_dir, mask_dir=None, transform=None):\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.images = sorted(os.listdir(img_dir))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_name = self.images[idx]\n        img_path = os.path.join(self.img_dir, img_name)\n        pil_img = Image.open(img_path).convert('RGB')\n        original_size = pil_img.size\n        image = self.transform(pil_img)\n\n        if self.mask_dir:\n            mask_path = os.path.join(self.mask_dir, img_name)\n            mask = Image.open(mask_path).convert('L').resize((1024, 512))\n            mask = (np.array(mask) > 127).astype(np.uint8)\n            mask = torch.tensor(mask, dtype=torch.float32)\n            return image, mask\n        else:\n            return image, img_name, original_size\n\n# ---------- ê°„ë‹¨í•œ CNN ëª¨ë¸ ì •ì˜ ----------\nclass SimpleCNNModel(nn.Module):\n    def __init__(self, n_class=1):\n        super().__init__()\n        # ì¸ì½”ë”\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(256, 128, 3, padding=1), nn.ReLU()\n        )\n        # ë””ì½”ë”\n        self.decoder = nn.Sequential(\n            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),\n            nn.ConvTranspose2d(128, 128, 2, 2), nn.ReLU(),\n            nn.Conv2d(128, 64, 3, padding=1), nn.ReLU(),\n            nn.ConvTranspose2d(64, 64, 2, 2), nn.ReLU(),\n            nn.Conv2d(64, 32, 3, padding=1), nn.ReLU()\n        )\n        # ë¶„ë¥˜ê¸°\n        self.classifier = nn.Conv2d(32, 1, kernel_size=1)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return self.classifier(x).squeeze(1)\n\n# ---------- Semi-Supervised í•™ìŠµ í•¨ìˆ˜ ----------\ndef train_semi(model, labeled_loader, unlabeled_loader, optimizer, device, lambda_u=0.5):\n    model.train()\n    total_loss = 0.0\n    unl_iter = iter(unlabeled_loader)\n    bce = nn.BCEWithLogitsLoss()\n    for imgs_l, masks in tqdm(labeled_loader, desc='Semi-Train'):\n        imgs_l, masks = imgs_l.to(device), masks.to(device)\n        # Supervised loss\n        out_l = model(imgs_l)\n        loss_sup = bce(out_l, masks) + dice_loss(out_l, masks)\n        # Unsupervised pseudo-labeling\n        try:\n            imgs_u, _, _ = next(unl_iter)\n        except StopIteration:\n            unl_iter = iter(unlabeled_loader)\n            imgs_u, _, _ = next(unl_iter)\n        imgs_u = imgs_u.to(device)\n        with torch.no_grad():\n            pseudo = torch.sigmoid(model(imgs_u))\n        mask_p = (pseudo > 0.9).float()\n        loss_unsup = bce(model(imgs_u), mask_p)\n        # Total loss\n        loss = loss_sup + lambda_u * loss_unsup\n        model.zero_grad()\n        loss.backward()\n        optimizer.step()\n        torch.cuda.empty_cache()\n        total_loss += loss.item()\n    return total_loss / len(labeled_loader)\n\n# ---------- RLE ì¸ì½”ë”© ----------\ndef mask_to_rle(mask):\n    pixels = mask.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(map(str, runs))\n\n# ---------- ë©”ì¸ ----------\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    transform = transforms.Compose([\n        transforms.Resize((512, 1024)),\n        transforms.ToTensor()\n    ])\n    # ë°ì´í„°ë¡œë”\n    labeled_ds = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/train/train/frames\",\n        \"/kaggle/input/lanesegmentationchallenge/train/train/lane-masks\",\n        transform\n    )\n    unlabeled_ds = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/test/test/frames\",\n        None, transform\n    )\n    labeled_loader = DataLoader(labeled_ds, batch_size=1, shuffle=True, num_workers=4)\n    unlabeled_loader = DataLoader(unlabeled_ds, batch_size=1, shuffle=True, num_workers=4)\n\n    # ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì €\n    model = SimpleCNNModel().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n    # Semi-supervised í•™ìŠµ\n    epochs = 3\n    print(\"â–¶ï¸ Semi-supervised í•™ìŠµ ì‹œì‘...\")\n    for ep in range(epochs):\n        loss = train_semi(model, labeled_loader, unlabeled_loader, optimizer, device)\n        print(f\"Epoch {ep+1}/{epochs} - Loss: {loss:.4f}\")\n    torch.save(model.state_dict(), \"semi_cnn_model.pth\")\n    print(\"ğŸ’¾ Semi-supervised ëª¨ë¸ ì €ì¥ ì™„ë£Œ\")\n\n    # --- í…ŒìŠ¤íŠ¸ ë° ì œì¶œ ---\n    print(\"ğŸ§ª submission.csv ìƒì„± ì‹œì‘...\")\n    model.eval()\n    results = []\n    os.makedirs('pred_masks', exist_ok=True)\n    with torch.no_grad():\n        for imgs, fnames, orig_size in tqdm(DataLoader(unlabeled_ds, batch_size=1), desc=\"ì¶”ë¡  ì¤‘\"):\n            imgs = imgs.to(device)\n            output = model(imgs)\n            pred_mask = (torch.sigmoid(output).squeeze(0).cpu().numpy() > 0.5).astype(np.uint8)\n            W, H = orig_size\n            pred_pil = Image.fromarray(pred_mask * 255)\n            resized_mask = pred_pil.resize((W, H), resample=Image.NEAREST)\n            resized_bin = np.array(resized_mask) > 127\n            rle = mask_to_rle(resized_bin.astype(np.uint8))\n            # ì˜ˆì¸¡ ë§ˆìŠ¤í¬ ì €ì¥\n            mask_path = os.path.join('pred_masks', fnames[0])\n            pred_pil.save(mask_path)\n            results.append({'filename': fnames[0], 'rle': rle})\n    pd.DataFrame(results).to_csv(\"submission.csv\", index=False)\n    print(\"âœ… submission.csv íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:48:52.883654Z","iopub.execute_input":"2025-06-19T19:48:52.884192Z","iopub.status.idle":"2025-06-19T19:48:52.938654Z","shell.execute_reply.started":"2025-06-19T19:48:52.884169Z","shell.execute_reply":"2025-06-19T19:48:52.937696Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2977570784.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/2977570784.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì €\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleCNNModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 33.12 MiB is free. Process 3330 has 15.85 GiB memory in use. Of the allocated memory 15.57 GiB is allocated by PyTorch, and 1.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 33.12 MiB is free. Process 3330 has 15.85 GiB memory in use. Of the allocated memory 15.57 GiB is allocated by PyTorch, and 1.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":19},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:51:41.232847Z","iopub.execute_input":"2025-06-19T19:51:41.233756Z","iopub.status.idle":"2025-06-19T19:51:41.238081Z","shell.execute_reply.started":"2025-06-19T19:51:41.233714Z","shell.execute_reply":"2025-06-19T19:51:41.237247Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\n# ---------- Dice Loss í•¨ìˆ˜ ì •ì˜ ----------\ndef dice_loss(pred, target, smooth=1e-6):\n    prob = torch.sigmoid(pred)\n    inter = (prob * target).sum(dim=(1, 2))\n    union = prob.sum(dim=(1, 2)) + target.sum(dim=(1, 2))\n    dice = (2 * inter + smooth) / (union + smooth)\n    return 1 - dice.mean()\n\n# ---------- ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ----------\nclass LaneDataset(Dataset):\n    def __init__(self, img_dir, mask_dir=None, transform=None):\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.images = sorted(os.listdir(img_dir))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_name = self.images[idx]\n        img = Image.open(os.path.join(self.img_dir, img_name)).convert('RGB')\n        img_t = self.transform(img) if self.transform else transforms.ToTensor()(img)\n        if self.mask_dir:\n            mask = Image.open(os.path.join(self.mask_dir, img_name)).convert('L')\n            mask = mask.resize((img_t.shape[2], img_t.shape[1]), Image.NEAREST)\n            mask_t = torch.from_numpy((np.array(mask) > 127).astype(np.float32))\n            return img_t, mask_t\n        else:\n            return img_t, img_name, img.size\n\n# ---------- ê²½ëŸ‰ CNN ëª¨ë¸ ì •ì˜ ----------\nclass LightSegNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # ì¸ì½”ë”\n        self.enc1 = nn.Sequential(\n            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.enc2 = nn.Sequential(\n            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.enc3 = nn.Sequential(\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU()\n        )\n        # ë””ì½”ë”\n        self.dec3 = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, 2, stride=2), nn.ReLU()\n        )\n        self.dec2 = nn.Sequential(\n            nn.ConvTranspose2d(32, 16, 2, stride=2), nn.ReLU()\n        )\n        self.classifier = nn.Conv2d(16, 1, 1)\n\n    def forward(self, x):\n        x1 = self.enc1(x)\n        x2 = self.enc2(x1)\n        x3 = self.enc3(x2)\n        d2 = self.dec3(x3)\n        d1 = self.dec2(d2)\n        out = self.classifier(d1)\n        return out.squeeze(1)\n\n# ---------- Semi-Supervised í•™ìŠµ í•¨ìˆ˜ ----------\ndef train_semi(model, labeled_loader, unlabeled_loader, optimizer, device, lambda_u=0.5):\n    model.train()\n    total_loss = 0.0\n    unl_iter = iter(unlabeled_loader)\n    bce = nn.BCEWithLogitsLoss()\n    for imgs_l, masks in tqdm(labeled_loader, desc='Semi-Train'):\n        imgs_l, masks = imgs_l.to(device), masks.to(device)\n        out_l = model(imgs_l)\n        loss_sup = bce(out_l, masks) + dice_loss(out_l, masks)\n        try:\n            imgs_u, _, _ = next(unl_iter)\n        except StopIteration:\n            unl_iter = iter(unlabeled_loader)\n            imgs_u, _, _ = next(unl_iter)\n        imgs_u = imgs_u.to(device)\n        with torch.no_grad():\n            pseudo = torch.sigmoid(model(imgs_u))\n        mask_p = (pseudo > 0.9).float()\n        loss_unsup = bce(model(imgs_u), mask_p)\n        loss = loss_sup + lambda_u * loss_unsup\n        model.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if device.type == 'cuda':\n            torch.cuda.empty_cache()\n        total_loss += loss.item()\n    return total_loss / len(labeled_loader)\n\n# ---------- RLE ì¸ì½”ë”© ----------\ndef mask_to_rle(mask):\n    pixels = mask.flatten(order='F')\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(map(str, runs))\n\n# ---------- ë©”ì¸ ----------\ndef main():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    transform = transforms.Compose([\n        transforms.Resize((256, 512)),\n        transforms.ToTensor()\n    ])\n    # ë°ì´í„°ë¡œë”\n    labeled_ds = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/train/train/frames\",\n        \"/kaggle/input/lanesegmentationchallenge/train/train/lane-masks\",\n        transform\n    )\n    unlabeled_ds = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/test/test/frames\",\n        None, transform\n    )\n    labeled_loader = DataLoader(labeled_ds, batch_size=1, shuffle=True, num_workers=2)\n    unlabeled_loader = DataLoader(unlabeled_ds, batch_size=1, shuffle=True, num_workers=2)\n\n    # ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì €\n    model = LightSegNet().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n\n    # Semi-supervised í•™ìŠµ\n    epochs = 10\n    print(\"â–¶ï¸ Semi-supervised í•™ìŠµ ì‹œì‘...\")\n    for ep in range(1, epochs+1):\n        loss = train_semi(model, labeled_loader, unlabeled_loader, optimizer, device)\n        print(f\"Epoch {ep}/{epochs} - Loss: {loss:.4f}\")\n    torch.save(model.state_dict(), \"semi_lightseg.pth\")\n    print(\"ğŸ’¾ Semi-supervised ëª¨ë¸ ì €ì¥ ì™„ë£Œ\")\n\n    # --- í…ŒìŠ¤íŠ¸ ë° ì œì¶œ ---\n    print(\"ğŸ§ª submission.csv ìƒì„± ì‹œì‘...\")\n    model.eval()\n    results = []\n    os.makedirs('pred_masks', exist_ok=True)\n    with torch.no_grad():\n        for imgs, fnames, orig_size in tqdm(DataLoader(unlabeled_ds, batch_size=1), desc=\"ì¶”ë¡  ì¤‘\"):\n            imgs = imgs.to(device)\n            out = model(imgs)\n            pred = (torch.sigmoid(out).squeeze(0).cpu().numpy() > 0.5).astype(np.uint8)\n            W, H = orig_size\n            pil = Image.fromarray(pred * 255)\n            pil.save(os.path.join('pred_masks', fnames[0]))\n            rle = mask_to_rle(pred)\n            results.append({'filename': fnames[0], 'rle': rle})\n    pd.DataFrame(results).to_csv(\"submission.csv\", index=False)\n    print(\"âœ… submission.csv íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:52:41.740951Z","iopub.execute_input":"2025-06-19T19:52:41.741301Z","iopub.status.idle":"2025-06-19T20:06:27.522412Z","shell.execute_reply.started":"2025-06-19T19:52:41.741282Z","shell.execute_reply":"2025-06-19T20:06:27.521662Z"}},"outputs":[{"name":"stdout","text":"â–¶ï¸ Semi-supervised í•™ìŠµ ì‹œì‘...\n","output_type":"stream"},{"name":"stderr","text":"Semi-Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3626/3626 [01:21<00:00, 44.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 - Loss: 0.9692\n","output_type":"stream"},{"name":"stderr","text":"Semi-Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3626/3626 [01:14<00:00, 48.66it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10 - Loss: 0.7978\n","output_type":"stream"},{"name":"stderr","text":"\nSemi-Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3626/3626 [01:15<00:00, 48.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10 - Loss: 0.7385\n","output_type":"stream"},{"name":"stderr","text":"Semi-Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3626/3626 [01:15<00:00, 48.33it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10 - Loss: 0.6986\n","output_type":"stream"},{"name":"stderr","text":"\nSemi-Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3626/3626 [01:16<00:00, 47.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10 - Loss: 0.6697\n","output_type":"stream"},{"name":"stderr","text":"Semi-Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3626/3626 [01:15<00:00, 48.09it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10 - Loss: 0.6493\n","output_type":"stream"},{"name":"stderr","text":"\nSemi-Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3626/3626 [01:14<00:00, 48.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10 - Loss: 0.6329\n","output_type":"stream"},{"name":"stderr","text":"Semi-Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3626/3626 [01:17<00:00, 46.75it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10 - Loss: 0.6191\n","output_type":"stream"},{"name":"stderr","text":"\nSemi-Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3626/3626 [01:17<00:00, 47.04it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10 - Loss: 0.6082\n","output_type":"stream"},{"name":"stderr","text":"\nSemi-Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3626/3626 [01:15<00:00, 47.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10 - Loss: 0.5978\nğŸ’¾ Semi-supervised ëª¨ë¸ ì €ì¥ ì™„ë£Œ\nğŸ§ª submission.csv ìƒì„± ì‹œì‘...\n","output_type":"stream"},{"name":"stderr","text":"ì¶”ë¡  ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2782/2782 [00:54<00:00, 50.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"âœ… submission.csv íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\n# ---------- Dice Loss í•¨ìˆ˜ ì •ì˜ ----------\ndef dice_loss(pred, target, smooth=1e-6):\n    prob = torch.sigmoid(pred)\n    inter = (prob * target).sum(dim=(1, 2))\n    union = prob.sum(dim=(1, 2)) + target.sum(dim=(1, 2))\n    dice = (2 * inter + smooth) / (union + smooth)\n    return 1 - dice.mean()\n\n# ---------- ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ----------\nclass LaneDataset(Dataset):\n    def __init__(self, img_dir, mask_dir=None, transform=None):\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.images = sorted(os.listdir(img_dir))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_name = self.images[idx]\n        img = Image.open(os.path.join(self.img_dir, img_name)).convert('RGB')\n        img_t = self.transform(img) if self.transform else transforms.ToTensor()(img)\n        if self.mask_dir:\n            mask = Image.open(os.path.join(self.mask_dir, img_name)).convert('L')\n            mask = mask.resize((img_t.shape[2], img_t.shape[1]), Image.NEAREST)\n            mask_t = torch.from_numpy((np.array(mask) > 127).astype(np.float32))\n            return img_t, mask_t\n        else:\n            return img_t, img_name, img.size\n\n# ---------- ê²½ëŸ‰ CNN ëª¨ë¸ ì •ì˜ ----------\nclass LightSegNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # ì¸ì½”ë”\n        self.enc1 = nn.Sequential(\n            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.enc2 = nn.Sequential(\n            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.enc3 = nn.Sequential(\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU()\n        )\n        # ë””ì½”ë”\n        self.dec3 = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, 2, stride=2), nn.ReLU()\n        )\n        self.dec2 = nn.Sequential(\n            nn.ConvTranspose2d(32, 16, 2, stride=2), nn.ReLU()\n        )\n        self.classifier = nn.Conv2d(16, 1, 1)\n\n    def forward(self, x):\n        x1 = self.enc1(x)\n        x2 = self.enc2(x1)\n        x3 = self.enc3(x2)\n        d2 = self.dec3(x3)\n        d1 = self.dec2(d2)\n        out = self.classifier(d1)\n        return out.squeeze(1)\ndef train_supervised(model, data_loader, optimizer, device):\n    model.train()\n    total_loss = 0.0\n    bce = nn.BCEWithLogitsLoss()\n    for imgs, masks in tqdm(data_loader, desc='Supervised-Train'):\n        imgs, masks = imgs.to(device), masks.to(device)\n        out = model(imgs)\n        loss = bce(out, masks) + dice_loss(out, masks)\n        model.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if device.type == 'cuda':\n            torch.cuda.empty_cache()\n        total_loss += loss.item()\n    return total_loss / len(data_loader)\ndef main():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    transform = transforms.Compose([\n        transforms.Resize((256, 512)),\n        transforms.ToTensor()\n    ])\n    # ë°ì´í„°ë¡œë”\n    labeled_ds = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/train/train/frames\",\n        \"/kaggle/input/lanesegmentationchallenge/train/train/lane-masks\",\n        transform\n    )\n    unlabeled_ds = LaneDataset(\n        \"/kaggle/input/lanesegmentationchallenge/test/test/frames\",\n        None, transform\n    )\n    labeled_loader = DataLoader(labeled_ds, batch_size=1, shuffle=True, num_workers=2)\n    unlabeled_loader = DataLoader(unlabeled_ds, batch_size=1, shuffle=True, num_workers=2)\n\n    # ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì €\n    model = LightSegNet().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n\n    # Semi-supervised í•™ìŠµ\n    epochs = 10\n    print(\"â–¶ï¸ Semi-supervised í•™ìŠµ ì‹œì‘...\")\n    for ep in range(1, epochs+1):\n        loss = train_supervised(model, labeled_loader, unlabeled_loader, optimizer, device)\n        print(f\"Epoch {ep}/{epochs} - Loss: {loss:.4f}\")\n    torch.save(model.state_dict(), \"semi_lightseg.pth\")\n    print(\"ğŸ’¾ supervised ëª¨ë¸ ì €ì¥ ì™„ë£Œ\")\n\n    # --- í…ŒìŠ¤íŠ¸ ë° ì œì¶œ ---\n    print(\"ğŸ§ª submission.csv ìƒì„± ì‹œì‘...\")\n    model.eval()\n    results = []\n    os.makedirs('pred_masks', exist_ok=True)\n    with torch.no_grad():\n        for imgs, fnames, orig_size in tqdm(DataLoader(unlabeled_ds, batch_size=1), desc=\"ì¶”ë¡  ì¤‘\"):\n            imgs = imgs.to(device)\n            out = model(imgs)\n            pred = (torch.sigmoid(out).squeeze(0).cpu().numpy() > 0.5).astype(np.uint8)\n            W, H = orig_size\n            pil = Image.fromarray(pred * 255)\n            pil.save(os.path.join('pred_masks', fnames[0]))\n            rle = mask_to_rle(pred)\n            results.append({'filename': fnames[0], 'rle': rle})\n    pd.DataFrame(results).to_csv(\"submission.csv\", index=False)\n    print(\"âœ… submission.csv íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}