{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-19T19:52:40.016891Z",
     "iopub.status.busy": "2025-06-19T19:52:40.016567Z",
     "iopub.status.idle": "2025-06-19T19:52:41.739277Z",
     "shell.execute_reply": "2025-06-19T19:52:41.738448Z",
     "shell.execute_reply.started": "2025-06-19T19:52:40.016865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T05:04:00.294481Z",
     "iopub.status.busy": "2025-06-20T05:04:00.293884Z",
     "iopub.status.idle": "2025-06-20T05:11:20.326687Z",
     "shell.execute_reply": "2025-06-20T05:11:20.325752Z",
     "shell.execute_reply.started": "2025-06-20T05:04:00.294456Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 528M/528M [00:06<00:00, 88.0MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂Ô∏è Training with VGG16 + Augmentation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 454/454 [05:19<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Loss: 0.4043\n",
      "‚ñ∂Ô∏è Inference and Submission...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2782/2782 [01:44<00:00, 26.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ aug_submission.csv created.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# ---------- Dice Loss ----------\n",
    "def dice_loss(pred, target, smooth=1e-6):\n",
    "    prob = torch.sigmoid(pred)\n",
    "    inter = (prob * target).sum(dim=(1,2))\n",
    "    union = prob.sum(dim=(1,2)) + target.sum(dim=(1,2))\n",
    "    return 1 - ((2 * inter + smooth) / (union + smooth)).mean()\n",
    "\n",
    "# ---------- Dataset ----------\n",
    "class LaneDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir=None, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.images = sorted(os.listdir(img_dir))\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.images[idx]\n",
    "        img = Image.open(os.path.join(self.img_dir, fname)).convert('RGB')\n",
    "        img_t = self.transform(img)\n",
    "        if self.mask_dir:\n",
    "            m = Image.open(os.path.join(self.mask_dir, fname)).convert('L')\n",
    "            m = m.resize((img_t.shape[2], img_t.shape[1]), Image.NEAREST)\n",
    "            mask = (np.array(m) > 127).astype(np.int64)\n",
    "            return img_t, torch.from_numpy(mask)\n",
    "        else:\n",
    "            return img_t, fname, img.size\n",
    "\n",
    "# ---------- VGG16 Segmentation Model ----------\n",
    "class VGG16Seg(nn.Module):\n",
    "    def __init__(self, n_class=2, pretrained=True):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg16(pretrained=pretrained)\n",
    "        self.encoder = vgg.features\n",
    "        self.classifier = nn.Conv2d(512, n_class, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        feat = self.encoder(x)\n",
    "        out = self.classifier(feat)\n",
    "        return nn.functional.interpolate(out, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "# ---------- RLE Encoding ----------\n",
    "def mask_to_rle(mask):\n",
    "    pixels = mask.flatten(order='F')\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "# ---------- Main Pipeline ----------\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Transforms with augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((512, 1024)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(0.3,0.3,0.3,0.1),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((512, 1024)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    # DataLoaders\n",
    "    train_imgs = \"lanesegmentationchallenge/train/train/frames\"\n",
    "    train_masks= \"lanesegmentationchallenge/train/train/lane-masks\"\n",
    "    test_imgs = \"lanesegmentationchallenge/test/test/frames\"\n",
    "    train_ds = LaneDataset(train_imgs, train_masks, train_transform)\n",
    "    train_ld = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=4)\n",
    "    test_ds  = LaneDataset(test_imgs, None, test_transform)\n",
    "    test_ld  = DataLoader(test_ds, batch_size=1, num_workers=2)\n",
    "    # Model, optimizer, loss\n",
    "    model = VGG16Seg().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    # Training\n",
    "    print(\"‚ñ∂Ô∏è Training with VGG16 + Augmentation...\")\n",
    "    for ep in range(1):\n",
    "        model.train(); tot=0\n",
    "        for imgs, masks in tqdm(train_ld, desc=f\"Epoch {ep}\"):\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            logits = model(imgs)\n",
    "            loss = 0.5 * ce_loss(logits, masks) + 0.5 * dice_loss(logits[:,1,:,:], masks==1)\n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "            tot += loss.item()\n",
    "        print(f\"Epoch {ep} - Loss: {tot/len(train_ld):.4f}\")\n",
    "    torch.save(model.state_dict(), 'vgg16_aug.pth')\n",
    "    # Inference & Submission\n",
    "    print(\"‚ñ∂Ô∏è Inference and Submission...\")\n",
    "    model.eval(); results = []\n",
    "    os.makedirs('pred_masks', exist_ok=True)\n",
    "    with torch.no_grad():\n",
    "        for imgs, fnames, orig in tqdm(test_ld, desc='Infer'):\n",
    "            imgs = imgs.to(device)\n",
    "            out = model(imgs)\n",
    "            pred = out.argmax(1).squeeze(0).cpu().numpy().astype(np.uint8)\n",
    "            pil = Image.fromarray(pred*255)\n",
    "            pil.save(os.path.join('pred_masks', fnames[0]))\n",
    "            results.append({'filename': fnames[0], 'rle': mask_to_rle(pred)})\n",
    "    pd.DataFrame(results).to_csv('submission.csv', index=False)\n",
    "    print(\"‚úÖ aug_submission.csv created.\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T21:19:03.233751Z",
     "iopub.status.busy": "2025-06-19T21:19:03.233130Z",
     "iopub.status.idle": "2025-06-19T21:22:12.921855Z",
     "shell.execute_reply": "2025-06-19T21:22:12.920860Z",
     "shell.execute_reply.started": "2025-06-19T21:19:03.233727Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂Ô∏è Î™®Îç∏ ÌïôÏäµ ÏãúÏûë...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 454/454 [01:33<00:00,  4.87it/s, loss=0.2442]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] ÌèâÍ∑† ÏÜêÏã§: 0.3259\n",
      "üíæ Î™®Îç∏Ïù¥ simple_cnn_model.pthÎ°ú Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.\n",
      "üß™ submission.csv ÏÉùÏÑ± ÏãúÏûë...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ï∂îÎ°† Ï§ë: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2782/2782 [01:35<00:00, 29.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ submission.csv ÌååÏùºÏù¥ Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§!\n"
     ]
    }
   ],
   "source": [
    "# ÌïÑÏöîÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# ---------- Dice Loss Ìï®Ïàò Ï†ïÏùò ----------\n",
    "# ÏòàÏ∏°Í≥º Ïã§Ï†ú Ï†ïÎãµ Í∞ÑÏùò Í≤πÏπòÎäî Ï†ïÎèÑÎ•º Ï∏°Ï†ïÌïòÎäî Dice Loss Ìï®Ïàò\n",
    "def dice_loss(pred, target, smooth=1e-6):\n",
    "    pred = torch.softmax(pred, dim=1)[:, 1, :, :]  # ÌÅ¥ÎûòÏä§ 1Ïóê ÎåÄÌïú softmax ÌôïÎ•†\n",
    "    target = (target == 1).float()                 # targetÎèÑ 1Ïù∏ ÌîΩÏÖÄÎßå ÏÑ†ÌÉù\n",
    "    intersection = (pred * target).sum(dim=(1, 2)) # ÍµêÏßëÌï©\n",
    "    union = pred.sum(dim=(1, 2)) + target.sum(dim=(1, 2)) # Ìï©ÏßëÌï©\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return 1 - dice.mean()\n",
    "\n",
    "# ---------- Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¥ÎûòÏä§ ----------\n",
    "class LaneDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir=None, transform=None):\n",
    "        self.img_dir = img_dir      # Ïù¥ÎØ∏ÏßÄ ÎîîÎ†âÌÜ†Î¶¨\n",
    "        self.mask_dir = mask_dir    # ÎßàÏä§ÌÅ¨ ÎîîÎ†âÌÜ†Î¶¨\n",
    "        self.images = sorted(os.listdir(img_dir))  # Ïù¥ÎØ∏ÏßÄ ÌååÏùº Ï†ïÎ†¨\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        pil_img = Image.open(img_path).convert('RGB')\n",
    "        original_size = pil_img.size  # (ÎÑàÎπÑ, ÎÜíÏù¥)\n",
    "        image = self.transform(pil_img)\n",
    "\n",
    "        if self.mask_dir:\n",
    "            # ÌïôÏäµ/Í≤ÄÏ¶ù Ïãú ÎßàÏä§ÌÅ¨ÎèÑ Ìï®Íªò Î∂àÎü¨Ïò¥\n",
    "            mask_path = os.path.join(self.mask_dir, img_name)\n",
    "            mask = Image.open(mask_path).convert('L').resize((1024, 512))\n",
    "            mask = (np.array(mask) > 127).astype(np.uint8)  # Ïù¥ÏßÑÌôî\n",
    "            mask = torch.tensor(mask, dtype=torch.long)\n",
    "            return image, mask\n",
    "        else:\n",
    "            # ÌÖåÏä§Ìä∏ Ïãú ÎßàÏä§ÌÅ¨ ÏóÜÏùå ‚Üí ÌååÏùºÎ™ÖÍ≥º ÏõêÎ≥∏ ÌÅ¨Í∏∞ Î∞òÌôò\n",
    "            return image, img_name, original_size\n",
    "\n",
    "# ---------- Í∞ÑÎã®Ìïú CNN Î™®Îç∏ Ï†ïÏùò ----------\n",
    "class SimpleCNNModel(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        # Ïù∏ÏΩîÎçî (ÌäπÏßï Ï∂îÏ∂ú)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        # ÎîîÏΩîÎçî (ÏóÖÏÉòÌîåÎßÅ)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, 3, padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 128, 2, 2), nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 64, 2, 2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, 3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        # ÌÅ¥ÎûòÏä§ Î∂ÑÎ•òÍ∏∞\n",
    "        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return self.classifier(x)\n",
    "# ---\n",
    "class VGG16Seg(nn.Module):\n",
    "    def __init__(self, n_class=2, pretrained=True):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg16(pretrained=pretrained)\n",
    "        self.encoder = vgg.features\n",
    "        self.classifier = nn.Conv2d(512, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(x)  # [B, n_class, H/32, W/32]\n",
    "        # Upsample back to input size\n",
    "        return nn.functional.interpolate(x, scale_factor=32, mode='bilinear', align_corners=False)\n",
    "# ---------- ÎßàÏä§ÌÅ¨Î•º RLEÎ°ú Ïù∏ÏΩîÎî©ÌïòÎäî Ìï®Ïàò ----------\n",
    "class RESNET16Seg(nn.Module):\n",
    "    def __init__(self, n_class=2, pretrained=True):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(pretrained=pretrained)\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.classifier = nn.Conv2d(512, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(x)  # [B, n_class, H/32, W/32]\n",
    "        # Upsample back to input size\n",
    "        return nn.functional.interpolate(x, scale_factor=32, mode='bilinear', align_corners=False)\n",
    "#--\n",
    "def mask_to_rle(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return \" \".join(map(str, runs))\n",
    "\n",
    "# ---------- Î©îÏù∏ Ìï®Ïàò ----------\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Ï†ïÏùò\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((512, 1024)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # --- ÌïôÏäµ ÏãúÏûë ---\n",
    "    train_dataset = LaneDataset(\n",
    "        \"lanesegmentationchallenge/train/train/frames\",\n",
    "        \"lanesegmentationchallenge/train/train/lane-masks\",\n",
    "        transform\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "    model = VGG16Seg(n_class=2).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    ce_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"‚ñ∂Ô∏è Î™®Îç∏ ÌïôÏäµ ÏãúÏûë...\")\n",
    "    for epoch in range(1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "        for imgs, masks in pbar:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            outputs = model(imgs)\n",
    "            ce = ce_loss_fn(outputs, masks)\n",
    "            d  = dice_loss(outputs, masks)\n",
    "            loss = 0.5 * ce + 0.5 * d  \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        print(f\"[Epoch {epoch+1}] ÌèâÍ∑† ÏÜêÏã§: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"simple_cnn_model.pth\")\n",
    "    print(\"üíæ Î™®Îç∏Ïù¥ simple_cnn_model.pthÎ°ú Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.\")\n",
    "\n",
    "    # --- ÌÖåÏä§Ìä∏ Ïù¥ÎØ∏ÏßÄÏóê ÎåÄÌïú Ï∂îÎ°† Î∞è Ï†úÏ∂ú ÌååÏùº ÏÉùÏÑ± ---\n",
    "    print(\"üß™ submission.csv ÏÉùÏÑ± ÏãúÏûë...\")\n",
    "    test_dataset = LaneDataset(\n",
    "        \"lanesegmentationchallenge/test/test/frames\",\n",
    "        None, transform\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, fnames, orig_size in tqdm(test_loader, desc=\"Ï∂îÎ°† Ï§ë\"):\n",
    "            imgs = imgs.to(device)\n",
    "            output = model(imgs)\n",
    "            pred_mask = output.argmax(1).squeeze(0).cpu().numpy().astype(np.uint8)\n",
    "            pred_pil = Image.fromarray(pred_mask)\n",
    "            W, H = orig_size\n",
    "            resized_mask = pred_pil.resize((W, H), resample=Image.NEAREST)\n",
    "            binary = np.array(resized_mask) > 0\n",
    "            rle = mask_to_rle(binary)\n",
    "            results.append({'filename': fnames[0], 'rle': rle})\n",
    "\n",
    "    pd.DataFrame(results).to_csv(\"aug_submission.csv\", index=False)\n",
    "    print(\"‚úÖ submission.csv ÌååÏùºÏù¥ Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§!\")\n",
    "\n",
    "# ÌîÑÎ°úÍ∑∏Îû® Ïã§Ìñâ ÏßÑÏûÖÏ†ê\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12341613,
     "sourceId": 102262,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
